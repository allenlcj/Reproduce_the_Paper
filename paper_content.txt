This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

1

APT-SAT: An Adaptive DNN Partitioning and
Task Offloading Framework within Collaborative
Satellite Computing Environments
Shifeng Peng, Zhishu Shen, Member, IEEE, Qiushi Zheng, Member, IEEE, Xuefeng Hou, Dawen Jiang,
Jingling Yuan, Senior Member, IEEE, and Jiong Jin, Member, IEEE
Abstract—Satellite computing has emerged as a promising technology for next-generation wireless networks, providing various data
processing capabilities within space networks. This advancement facilitates the widespread implementation of artificial intelligence
(AI)-driven IoT applications like image processing tasks involving deep neural networks (DNN). However, the constrained
computational and communication capacities of individual satellites pose significant challenges in efficiently managing DNN tasks
generated by diverse users. One viable solution involves partitioning DNN tasks into multiple subtasks and distributing them across
multiple satellites for collaborative computing. Despite its potential, it faces challenges in effectively partitioning DNNs to minimize delay
and energy consumption while ensuring efficient subtask allocation to maintain load balancing and high task completion rates. To this
end, we propose an adaptive DNN partitioning and task offloading framework named APT-SAT to enable efficient distributed computing
among multiple satellites. APT-SAT incorporates an adaptive DNN partitioning algorithm that aims to evenly distribute the workload of
DNN slices. Furthermore, a routing and task offloading algorithm, leveraging the soft actor-critic (SAC) approach, is introduced to
optimize offloading decisions. The extensive experiments validate the effectiveness of APT-SAT in terms of task completion rate, delay,
energy consumption, and resource utilization, highlighting its potential for enabling efficient satellite-based AI applications.
Index Terms—Collaborative computing, satellite networks, DNN partitioning, task offloading

✦

1

I NTRODUCTION

T

HE beyond 5G or 6G (B5G/6G) networks are anticipated to meet the ubiquitous demands of the future
digital society. Compared to the previous generations of
communication standards, B5G/6G has the characteristics
of employing large bandwidth, low latency, and wide connection, facilitating enhanced mobile broadband and Internet of Things (IoT) services for a vast number of users [1],
[2]. However, the existing base stations (BSs) are unrealistic
for widespread deployment in remote rural areas, primarily
due to geographical and economic restrictions. Furthermore,
the proliferation of mobile devices and the growing demand
for smart IoT applications in these areas have given rise to
a surge in computationally intensive IoT-based tasks. In the
absence of robust ground network infrastructure, processing
these tasks generated by users or devices in remote rural
areas becomes a critical issue [3].

Shifeng Peng, Zhishu Shen, Xuefeng Hou, Dawen Jiang, and Jingling
Yuan are with the School of Computer Science and Artificial Intelligence,
Wuhan University of Technology, Wuhan, China (e-mail: psf@whut.edu.cn,
z shen@ieee.org, {martinhou, davin, yjl}@whut.edu.cn). Zhishu Shen and
Jingling Yuan are also with the Hubei Key Laboratory of Transportation
Internet of Things, School of Computer Science and Artificial Intelligence,
Wuhan University of Technology, Wuhan, China, and China–Chile ICT Belt
and Road Joint Laboratory.
Qiushi Zheng and Jiong Jin are with the School of Engineering,
Swinburne University of Technology, Australia (e-mail: {qiushizheng,
jiongjin}@swin.edu.au).
This work was supported in part by the National Natural Science Foundation
of China (Grant No. 62472332) and the Hubei Provincial International
Science and Technology Cooperation Project (No. 2024EHA031).
Corresponding author: Zhishu Shen.

Satellite computing has emerged as a potential technology in addressing the aforementioned issue occurring in remote rural areas. It possesses a unique strength in providing
global coverage, extending connectivity to areas beyond the
reach of conventional terrestrial networks [4]–[6]. Leveraging on-board computing capabilities in satellites for various
IoT task processing can notably reduce the service delay
and energy consumption [7], especially for image processing
tasks involving deep neural network (DNN). Unfortunately,
these DNN tasks are inherently computation-intensive and
delay/energy-critical inference tasks [8]. As a result, it is
challenging to execute these tasks on resource-constrained
satellites with low delay and energy consumption. Partitioning the DNN model into multiple segments, with a
focus on minimizing transmission costs, can enhance its
inference performance [9], [10]. Based on this observation,
Chen et al. proposed a practical approach involves partitioning DNN computations between satellites to enable
collaborative satellite computing [11]. In this approach, the
task segments corresponding to DNN slices are offloaded
to different satellites for collaborative computing. However,
due to the different execution delays, energy consumption,
and intermediate output data sizes of each DNN layer, collaborative computing methods between satellites may lead
to long-term transmission of massive intermediate data, resulting in unpredictable communication delays and energy
consumption [12]. Furthermore, to enhance feature representation, DNN models frequently incorporate a substantial
number of layers [13], thereby intensifying the complexity
of DNN partitioning process.
On the other hand, task offloading schemes are also

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

2

crucial for the efficient processing of DNN tasks on satellite
networks. Currently, the task offloading scheme predominantly employs a centralized architecture [14]. In this architecture, a single satellite dictates the offloading locations for
all tasks [15], which results in substantial communication
and computational overhead, particularly within large-scale
low Earth orbit (LEO) satellite networks. Moreover, the centralized architecture can lead to unbalanced use of satellite
resources, resulting in overburdened computing loads on
certain satellites [16]. To solve this issue, a distributed architecture that relies on data sharing among various satellites
is anticipated. In this architecture, each terminal, such as
a satellite, independently determines offloading decisions
based on its local observations [17]. Despite these efforts,
issues with non-convergence arise when striving for global
optimization. A key inquiry is whether we can enhance
the optimization of DNN collaborative computing within
satellite networks.
To this end, we propose APT-SAT, an Adaptive DNN
Partitioning and Task offloading framework within collaborative SATellite computing environments. This framework
aims to decrease the delay and energy consumption associated with DNN task execution, enhance task completion
rate and promote satellite load balancing. Initially, APT-SAT
utilizes prediction models to facilitate splitting the DNN
tasks of varying sizes. These models forecast the delay and
energy usage of DNN model layers within a simulated
satellite network setting. Subsequently, it identifies the most
suitable partitioning points for DNN models based on these
predictions and network conditions. The decision satellite
then splits the DNN task into segments at these points
and allocates them to multiple satellites for collaborative
computing. Based on this framework, we develop an adaptive DNN partitioning algorithm, with the aim of selecting
multiple partition points to reduce the execution and transmission delay and energy consumption. In addition, we introduce an algorithm developed from soft actor-critic (SAC)
approach to explore optimal solutions in a large decision
space. The primary contributions of this work include:
•

•

•

A framework APT-SAT is developed to facilitate
collaborative computing between multiple satellites.
This framework encompasses the prediction stage,
the partitioning stage, and the collaborative computing stage, focusing on optimizing task completion
rate, computation delay and energy consumption.
Based on this framework, an adaptive DNN partitioning algorithm is developed for identifying optimal multiple partitioning points of the DNN model
that meet the requirements of multiple network criteria. Moreover, a self-adaptive task offloading algorithm based on SAC is introduced to determine
the optimal offloading decision in dynamic network
environments.
Extensive experiments are conducted on two typical DNN models with different network scales. The
obtained results highlight the superior performance
of our proposal against other methods in terms of
task completion rate, task delay, and task energy
consumption.

The remainder of this paper is organized as follows:

Fig. 1: Typical DNN partitioning schemes.
TABLE 1
C OMPARISONS OF DNN PARTITIONING MODELS .
Models
DeepThings [18]
Neurosurgeon [19]
MoDNN [20]
Cogent [21]
DeepX [22]
AOFL [23]
CRIME [24]
DeepSlicing [25]
Edgent [26]
This work
1
2

PTN1
Vertical
Horizontal
Horizontal
Horizontal
Hybrid
Hybrid
Hybrid
Hybrid
Hybrid
Horizontal

Criteria
Memory, cost
Energy, delay
Delay
Accuracy, delay
Energy, memory
Latency, cost
Energy, delay
Memory, delay
Accuracy
Energy, delay, VW2

Slice Num.
≥2
2
≥2
2
≥2
≥2
≥2
≥2
2
≥2

PTN: Partitioning scheme
VW: Variance of workload

Section 2 summarizes the research background and related
work and Section 3 describes the problem statement. Section 4 summarizes the framework we proposed and presents
our proposed DNN partitioning and task offloading algorithms. Section 5 summarizes the evaluation results that
verify the performance of APT-SAT against that of the
comparable methods. Section 6 concludes this paper.

2

BACKGROUND AND R ELATED W ORK

2.1

DNN Partitioning and Task Splitting

The characteristics of DNN models enable them to be easily
partitioned and deployed to multiple satellites for collaborative computing. As illustrated in Fig. 1, the partitioning
scheme of the DNN models can be divided into vertical
partitioning, horizontal partitioning, and hybrid (verticalhorizontal) partitioning. By applying these partitioning
schemes to the given DNN model, the original model is
divided into interdependent slices of varying granularity.
These slices are then deployed across different devices, such
as edge devices and satellites, based on their dependency
relationships.
Table 1 summarizes typical models that utilize various
DNN partitioning schemes. These models optimize critical
performance metrics, such as latency, energy consumption,
and accuracy, by identifying partition points within the
model that best align with the user or system requirements.
For example, Li et al. [26] introduced an on-demand deep
learning model inference framework that operates across
devices and edge clouds. It enhances deep learning inference efficiency through methods like DNN partitioning and
DNN rightsizing. Zhang et al. [25] proposed DeepSlicing, a
technique that supports customized, flexible, fine-grained

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

3

partitioning to enable collaborative and adaptive DNN
inference. In this approach, both DNN models and data
are partitioned to balance computation and synchronization
effectively. Kim et al. [27] introduced CO-PILOT, a partition
framework for collaborative intelligence aiming at reducing
end-to-end latency, while maintaining model accuracy. Li et
al. [28] investigated distributed DNN inference with finegrained model partitioning to find a competitive model
partitioning policy of per DNN blocks that reduces DNN
inference delay. Liao et al. [29] proposed a partition point
retain algorithm to reduce solution space and an optimal
partition point algorithm to find the optimal partitioning
point with the minimum cost for each edge server corresponding to each mobile device. Liang et al. [30] proposed
a dynamic adaptive DNN surgery scheme, which optimally
partitions the DNN under different network conditions. It
employs the graph minimum cut algorithm to determine
the optimal partition point between the cloud and the edge.
The partitioning methods utilized in the aforementioned
works primarily employ coarse-grained approaches, like
one-point or two-point partitioning. The fine-grained partitioning method has not been considered, which is crucial
for multi-equipment collaborative computing. On the other
hand, whether the workload of each block is balanced after
DNN partitioning has not been considered. Balancing the
workload of each piece of work can maximize the utilization
of resources of the equipment participating in the task
execution. Our proposed framework supports horizontal
partitioning by enabling layer-wise splitting across all types
of DNN layers, thereby facilitating fine-grained hierarchical
decomposition and collaborative computing in large-scale
satellite networks. In contrast, designing a vertical or hybrid
partitioning strategy, where DNN tasks are divided into
grid-based subtasks for parallel execution, it is essential to
consider the computational overhead in dynamic resourceconstrained satellite networks.
2.2

Satellite Computing

Due to the lack of onboard computing capabilities, traditional satellite systems have long relied on the bent-pipe architecture, whose fundamental limitation lies in their dependency on ground systems for data processing. As a result,
current satellite systems primarily serve data collection and
communication functions. A typical example is the CryoSat2 satellite [31], which conducts precise measurements of
Earth’s cryosphere for environmental monitoring. Instead
of performing advanced processing in orbit, it transmits
raw observational data to terrestrial computing infrastructure for complex analysis. From a theoretical perspective,
Denby et al. [32] demonstrated that as the network scale
exceeds thousands of satellites, control command interactions and data communication introduce significant latency,
downlink deficits, and energy consumption challenges. Similarly, Bhattacherjee et al. [33] indicated that the reliance
on ground station-dependent data processing in traditional
satellite systems creates overwhelming bandwidth pressure
on satellite-ground links.
With the rapid deployment of LEO satellite constellations, satellite computing, an essential technical paradigm
supporting diverse AI-driven tasks, is swiftly transitioning

from theoretical exploration to real-world implementation.
Specifically, the onboard computing addresses the challenges of the traditional satellite systems by enabling inorbit data processing and feature extraction, significantly
reducing the volume of transmitted data. This paradigm not
only decreases reliance on ground links but also enhances
transmission efficiency [5], [34]. Beyond mitigating bandwidth pressure on satellite-ground links, these advancements also pave the way for space-based edge computing: Cassará et al. evaluated the global service coverage
performance of LEO satellites functioning as distributed
computing nodes through task offloading mechanisms [35].
However, the practical implementation of satellite computing faces multiple physical constraints and technical
bottlenecks: (1) The dynamic topology caused by satellites’
high-speed motion introduces instability in communication
latency [36], and (2) the impact of space radiation on the reliability of commercial off-the-shelf (COTS) components. The
European Space Agency (ESA)’s CloudScout project [37]
reduced in-orbit inference errors in deep learning models
by employing FPGA accelerators and radiation-hardened
design. Unfortunately, the high hardware costs remain a
challenge for large-scale satellite deployment.
Recently developed satellite platforms equipped with
onboard edge computing capabilities offer significant advantages, including real-time data processing and a reduced
volume of data transmission to ground stations. For example, a research team at Carnegie Mellon University [38]
introduced an Onboard Edge Computing (OEC) architecture
designed for camera-equipped nano-satellites. This architecture enables onboard processing of sensed data, including
the execution of machine learning and inference algorithms,
significantly reducing system latency. In addition, a research
group of Beijing University of Posts and Telecommunications [39] developed an open satellite research platform
named Tiansuan, which provides satellite networks with
an on-orbit lightweight 5G core and edge computing platform. By leveraging KubeEdge and its AI extension, this
platform facilitates satellite-ground collaborative inference
applications, achieving up to a 50% improvement in inorbit image detection accuracy while reducing satellite data
transmission by 90%.
2.3

Task Offloading in Satellite Networks

As aforementioned, satellite computing is an emerging research field and how to make the optimal strategy in a
hierarchical computation offloading architecture is one of
the main challenges. Task offloading in satellite networks
involves redistributing computational workloads, such as
data processing and AI inference, from resource-constrained
nodes to neighboring nodes within the network. This process optimizes network performance by balancing key factors such as latency, energy consumption, bandwidth availability, and computational capacity.
Table 2 summarizes the existing task offloading algorithms designed for satellite networks. Among these algorithms, Cheng et al. [40] conducted a joint optimization of computation offloading and power control. They
introduced a Lyapunov optimization-based algorithm for
minimizing the overall delay of tasks while satisfying the

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

4

TABLE 2
C OMPARISONS OF TASK OFFLOADING ALGORITHMS .
Ref.
[40]
[41]
[42]
[43]
[44]
[45]
[46]
[47]
[48]
[49]
This work

Methods
Lyapunov
ADMM
TTCO
LSTM
MA-DATD3
MADDPG
MAIBJ
GA
DDPG
DQN
SAC

Energy
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓

Delay
✓

Task drop rate

✓
✓
✓
✓
✓
✓
✓

✓
✓
✓

energy constraints of satellites. Tang et al. [41] transformed
the offloading decision optimization problem into a linear programming problem by using the binary variables
relaxation method. A distributed algorithm based on the
alternating direction method of multipliers (ADMMs) was
proposed to approximate the optimal solution with low
computational complexity. Zhang et al. [50] explored a partial computation offloading strategy to minimize system
energy consumption, which included joint considerations
of user association, power allocation, task scheduling, and
bandwidth assignment. Peng et al. [47] proposed a task
offloading method developed from genetic algorithm (GA)
that optimizes offloading decisions by analyzing the task
inference process for collaborative satellite computing systems.
In recent years, the advancement of deep reinforcement learning (DRL) technologies has led to their growing
adoption in task offloading within satellite networks. Ji et
al. [51] proposed a multi-agent double actors twin delayed
deterministic policy gradient (MA-DATD3) algorithm that
contains double actors and double critics. MA-DATD3 is
designed to solve the computation offloading optimization
problem with a centralized training and decentralized execution paradigm. Li et al. [52] proposed a multi-agent deep
reinforcement learning algorithm with global rewards to
optimize the offloading decision via a decentralized method,
thus achieving the computation offloading and resource
allocation for the LEO satellite network. Lyu et al. [46]
proposed a multi-agent information broadcasting and judging (MAIBJ) algorithm. With MAIBJ, satellites can decide
whether to utilize the received information or not, which enhances the flexibility of offloading decision making. Zhang
et al. [48] proposed a collaborative computation offloading scheme based on deep deterministic policy gradient
(DDPG), with the objective to minimize the system energy
consumption during long-term task processing in satellite
edge computing environments. However, these methods
primarily catered to scenarios with a limited number of
satellites covering a single area, rendering them unsuitable
for offloading extensive DNN tasks that demand substantial
computing resources.

3

P ROBLEM S TATEMENT

3.1

System Model

In this paper, we consider an integrated satellite-ground system comprising LEO satellites and user equipments (UEs)
distributed across remote rural areas. A network of multiple
LEO satellites is deployed to achieve seamless global coverage as shown in Fig. 2. In this satellite network, each satellite
i (∈ S ) orbits the Earth periodically, enabling continuous
satellite-ground connections. Due to the limited computational capabilities of UEs, these devices are constrained in
handling resource-intensive tasks. Consequently, the computing tasks need to be offloaded to satellites via gateways
for processing, with all task transmissions required to pass
through the gateways in the system [53]. The gateway g (∈
G ) in each area is responsible for collecting tasks generated
by the UEs within its designated area and transmitting these
tasks to the satellite through wireless links. The satellites,
equipped with pre-trained deep neural network (DNN)
models, process the received tasks. Moreover, satellite-tosatellite communication is facilitated through inter-satellite
link (ISL). For example, in Fig. 2, Satellite #1 receives tasks
to be conducted by DNN model 1 from a specific area
and executes task splitting. The segmented tasks are then
offloaded to Satellite #4 and Satellite #5 through ISL (See
the red arrow in Fig. 2).
Without loss of generality, we designate the decisionmaking satellite to provide access and computing services to
the UEs and gateway g within its covered area. We assume
the gateway prioritizes satellites within its current coverage
based on orbital dynamics to minimize latency and maintain
reliable line-of-sight connectivity. The system operational
time is divided into Γ discrete time slots of unit length,
denoted by τ = {1, 2, ..., Γ}. Hypothetically, the number
of DNN task arrivals within the coverage of the decisionmaking satellite in each slot t (∈ τ ) follows a Poisson
distribution. Upon receiving the DNN tasks, the decisionmaking satellite splits them and offloads these segments to
multiple satellites for collaborative computing.

3.2

Communication Model

Before the satellite determines task offloading decisions,
the tasks must be transmitted to the satellite through the
satellite-ground wireless link. Assuming that multiple gateways share the available bandwidth without causing interference, the average transmission rate vg,i (t) between
gateway g and satellite i is determined using the Shannon
formula as follows:

vg,i (t) = B0 log2 (1 +

Pg ξg,i (t)
),
GN

(1)

where B0 is the channel bandwidth, Pg represents the
transmit power, ξg,i (t) denotes the channel gain consists of
large-scale fading and shadowed-Rician fading [41], and GN
indicates the additive white Gaussian noise.
The satellite network consists of No orbits, on which Ns
satellites are uniformly distributed across these orbits. Due
to the constraints imposed by communication distances,
each satellite can only transmit tasks to its adjacent satellites
through ISL. Assuming Gaussian channels, the maximum

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

5
•

Satellite computing resources: we define the computing
resource occupancy W of the satellite after loading
the new DNN task slice as:

W = q + mk , k ∈ {1, 2, ..., L},

(4)

where q represents the computing resources currently occupied by the satellite for existing DNN task
slices, and mk represents the computing resources
required for each new DNN task slice. The maximum
computing resource capacity of the satellite is Mw . If
W < Mw , the task slices are loaded onto the satellite
for processing; otherwise, the respective task slice is
discarded.
•

Satellite communication resources: we define the communication resource occupancy U after satellite
transmission of intermediate data as follows:

Fig. 2: System model.

U = b + bk , k ∈ {1, 2, ..., L},

where b represents the communication resources that
the satellite is currently occupying. The maximum
communication resource capacity of the satellite is
Mb , bk represents the communication resources allocated to each DNN task segment. If U < Mb , the
data can be transferred and the segment will proceed,
otherwise, the segment is discarded.

achievable data rate r(i, j) for the transmission between
satellites i and j is [54]:

Pt Gi (j)Gj (i)Fi (j)Fj (i)
),
(2)
kΘB
where B is the bandwidth between satellites, Pt is the
transmission power, Gi (j) and Gj (i) are the gain of the
transmit and received antennas, respectively. Fi (j), Fj (i)
represent the beam pointing coefficient (Fi (j), Fj (i) < 1),
k is the Boltzmann constant, and Θ is the equivalent noise
temperature.
r(i, j) = B log2 (1 +

3.3

Computation Model

Each satellite is assumed to be equipped with a predefined
DNN model, enabling it to handle specific DNN task segments. Additionally, a satellite can transmit its output to
adjacent satellites for further processing, such as inference
and computations like pooling and convolution for the next
slice. Given the constraints on satellite resource capacity and
the size of DNN task segments, optimizing the utilization of
each satellite’s computing resources is crucial. This requires
achieving a balanced workload distribution across satellites
for each DNN task segment. In this paper, we evaluate the
balance of workload distribution across the satellite network
using the variance of workload for each satellite as a metric.
The variance function var is defined as:
PL
(zk − z̄)2
, k ∈ {1, 2, ..., L},
(3)
var = k=1
L
where zk represents the workload of each satellite, z̄ corresponds to the average workload across all satellites, and L
specifies the expected sliced number.
During the operation of a satellite network, critical situations may occur when a satellite’s available computing
resources or communication resources approach exhaustion.
In both cases, the DNN mission segment assigned to the
corresponding satellite cannot be processed, leading to the
potential loss of the original mission. For this reason, it is
significant to determine whether the task can be processed
on the satellite based on the current computing resources
and communication resource usage.

(5)

3.4

Task Delay, Energy and Task Drop Model

The task delay comprises computational delay and transcomp
mission delay. For each pre-split DNN task, let qi,j,l denote
the workload (computational data size) for slice l of j-th task
in satellite i, and let Cxl represent the computing resources
allocated by the satellite x which executes task slice l, where
comp
the computation delay ti,j,l is calculated by:
comp
qi,j,l
comp
ti,j,l =
.
Cxl

(6)

Let L be the expected sliced number, and NiTK
be the number of DNN tasks in satellite i. The decomp comp
comp
lays {ti,j,1 , ti,j,2 , · · · , ti,j,L } are associated with the
slices of the j-th block processed by satellite i, while
{si,j,1 , si,j,2 , · · · , si,j,L } denote the satellites executing the
comp
corresponding slices. The computation delay tx of satellite
x can be deduced by:
TK

tcomp
=
x

Ni
L
XX
X
tcomp
i,j,k · Πsi,j,k =x ,

(7)

i∈S j=1 k=1

where Πsi,j,k =x is a binary indicator function that takes the
value 1 if si,j,k = x and 0 otherwise.
For the transmission delay, let MH(i, j) be the Manhattan
distance between satellite i and satellite j . MH(i, j) confines
satellite communications within a predefined maximum
range, thereby simplifying path planning and bounding
tran
communication overhead to a predefined range. Let qi,j,k
denote the workload of intermediate data for slice k of j-th
task in satellite i. The transmission delay ttran
x of satellite x is
calculated by:
TK

ttran
x =

Ni L−1
XX
X

tran
MH(si,j,k , si,j,k+1 ) · qi,j,k
· Πsi,j,k =x . (8)

i∈S j=1 k=1

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

6

The total delay tsum
x of satellite x is defined as:
comp
tsum
+ ttran
x = tx
x .

(9)

Similarly, regarding the task energy consumption, we
adopt the widely used model of energy consumption per
computing cycle as e = κf 2 [41], where κ represents the
energy consumption coefficient depending on the effective
switching capacitance of the chip architecture and f denotes
the CPU frequency (i.e., the computational capability of
comp
satellite). So the energy consumption ei,j,l for slice l of j-th
task in satellite i is calculated by:
2

l
ecomp
i,j,l = κCx dl cl ,

(10)

where dl denotes the size of the computation data and cl
indicates the task workload of slice l. This indicates that the
slice l consists of dl bits, and its computational workload is
cl cycles/bits.
comp comp
comp
Let {ei,j,1 , ei,j,2 , · · · , ei,j,L } be the energy consumption of slices of j-th task of satellite i. The computation
comp
energy consumption ex of satellite x can be deduced by:
TK

ecomp
=
x

Ni
L
XX
X

ecomp
i,j,k · Πsi,j,k =x .

(11)

i∈S j=1 k=1

For the transmission energy consumption, let ps be the
transmission power between satellites. The transmission
energy consumption etran
of satellite x is calculated by:
x
TK

etran
x =

Ni L−1
XX
X

tran
ps · MH(si,j,k , si,j,k+1 ) · qi,j,k
· Πsi,j,k =x .

maximum allowable delay. Second, the total computational
resources allocated to all scheduled DNN tasks must remain within the available computational capacity of the
current satellite (See Eq. 4). Similarly, the total communication resources allocated to all scheduled DNN tasks
must not surpass the available communication capacity of
the current satellite (See Eq. 5). Additionally, each satellite
must allocate a non-negative amount of computational and
communication resources to the DNN task associated with
it during time slot t. However, to reduce the service delay,
it is undesirable to transmit tasks to satellites located far
away. So for any offloading scheme that aims to choose candidate satellites si from the task offloading decision space
Ax determined by satellite x, the constraint is expressed
in Eq. 16a. The Manhattan distance between x and si ,
denoted as MH(x, si ), must be strictly less than DM , where
DM represents the maximum permissible communication
distance.
Besides, the final offloading scheme is defined by a
sequence of satellites (s1 , s2 , · · · , sL ), with each satellite
assigned to execute its respective task segment. The arriving
tasks can either be completed according to a predefined
scheme (s1 , s2 , · · · , sL ) or be dropped during progressing.
The drop point dpi,j , which denotes the dropping point
of j-th task block for satellite i, is constrained to the set
{1, 2, · · · , L} (See Eq. 16b). The offloading process is completed when dpi,j = L + 1. From the perspective of DNN
partitioning, the number of DNN layers Nl must be strictly
larger than a preset partitioned number L, as described in
Eq. 16c.

i∈S j=1 k=1

(12)
can be
The satellite x’s total energy consumption esum
x
expressed as the sum of two types of energy as:
comp
esum
+ etran
x = ex
x .

(13)

In addition, the task drop occurs when the computing
or communication resources of the current satellite are insufficient to handle the assigned workload. Let Di be the
number of drop of satellite i, the task total drop rate rD is
calculated as:
P
Di
i∈S
rD = P TK .
(14)
Ni
i∈S

3.5

Optimization Objective

Based on the aforementioned models, we aim to minimize
the task drop rate, delay, and energy consumption during
the whole process. The optimization problem is defined
below:
X
X
min(rD +
tsum
+
esum
(15)
i
i ),
i∈S

i∈S

s.t.
MH(x, si ) ≤ DM , ∀si ∈ Ax ,

dpi,j ∈ {1, 2, · · · , L + 1},
Nl ≥ L,

∀i ∈ S, 1 ≤ j ≤ NiTK ,

∀i ∈ S, 1 ≤ j ≤ NiTK .

(16a)
(16b)
(16c)

Regarding the constraints, the following conditions must
be satisfied: First, the task delay must not exceed the
designated QoS (Quality of Service) requirement for the

4 A N A DAPTIVE DNN PARTITIONING AND TASK
O FFLOADING F RAMEWORK
4.1

Framework Overview

The offloading decisions are associated with the coupling
relationship among different variables. Even in the case of a
single-objective optimization problem, it remains a discrete
and non-convex problem, which is regarded as a classical
NP-hard problem [41]. To achieve the optimization objective
in Eq. 15, we propose an adaptive DNN partitioning and
task offloading framework APT-SAT, designed to enable collaborative computing between multiple satellites. As shown
in Fig. 3, this framework consists of three stages: the prediction stage, the partitioning stage, and the collaborative inference stage. Utilizing the proposed framework, the satellite
distributes tasks into different blocks and splits them into
L segments at designated partition points. The cooperative
processing sequence among satellites is determined by the
selected task offloading scheme, ensuring that each satellite
executes task processing based on the computed decision.
The overview of these three stages is summarized below:
Prediction Stage: Using regression models, this stage estimates layer-wise inference delay and energy consumption
of DNNs. The obtained delay and energy prediction results
are then used for the next DNN partitioning stage.
Partition Stage: Introducing an adaptive DNN partitioning algorithm, this stage divides DNN computations
into several blocks. This algorithm considers various factors including satellite bandwidth, transmission power and

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

7

Fig. 3: Overview of the proposed framework APT-SAT.
satellite computing capacity to determine optimal partition
points.
Collaborative Computing Stage: Offloading multiple
DNN blocks to satellites for collaborative computing, this
stage leverages a task offloading algorithm developed from
SAC. Due to the presence of multiple satellites simultaneously collecting tasks from the ground, APT-SAT selects an
appropriate offloading sequence for the satellites, ultimately
reducing total inference delay and energy consumption,
increasing the task completion rate, and achieving workload balancing across the computing resources of multiple
satellites.

layer types. The equations are formulated as follows:
X
yt = α0 +
αi xi

ye = β0 +

Prediction Stage

X

β i xi

(18)

∀i

where α0 and β0 are intercept terms (constants), αi and βi
are regression coefficients.
The prediction results will then serve as reference data
for the subsequent DNN partitioning stage.
4.3

4.2

(17)

∀i

Partition Stage

Algorithm 1 Adaptive DNN Partitioning Algorithm

The primary goal of this stage is to analyze the inference latency and energy consumption of DNN layers within satellite environments. In this stage, we consider four distinct
types of DNN layers: convolutional layers, fully connected
layers, activation layers, and pooling layers.
It is worth noting that the analysis involves recording
the inference delay and energy consumption of individual
layer, rather than the entire model. Based on the analysis, we
develop prediction models for each type of DNN layer using
multiple linear regression (MLR)-based approach. Inspired
by [19], [26], the independent variables in these models are
carefully selected based on the unique characteristics of each
layer type: For the convolution layer, the regression model
uses the number of features in the input feature maps,
and the term (filter size/stride)2 × (the number of filters), representing the computation per pixel in the input feature
map. For fully-connected and pooling layers, the regression
model relies on the sizes of the input and output feature
maps. For activation layer, input data size is used in the
regression model. Based on these feature variables xi , linear
regression equations for predicting layer latency yt and
energy consumption ye can be constructed for the four DNN

Input: wk , texec
, ttrans
, eexec
, etrans
, L.
k
k
k
k
Output: The partitioning result with L − 1 points.
1: Initialize all criteria and parameters, combinations of
partition points CPP ← combinations(Nl , L − 1).
2: for i = 1, 2, · · · , |CPP| do
3:
Calculate variance of workload VAR, total delay T
and total energy consumption E by:
1
CPP
Nl
Pi
P
VAR ← var(
wk , · · · ,
wk )
k=1

T ←

CPP1
i

P

k=1

E←

1
CPP
Pi

k=1

+ ··· +
texec
k
+ ··· +
eexec
k

k=CPPL−1
i
Nl
P
k=CPPL−1
i
Nl
P
k=CPPL−1
i

texec
+
k

L−1
P
k=1

eexec
+
k

ttrans
CPPk

L−1
P
k=1

i

etrans
CPPk
i

dvtotal ← θ · VAR + δ · T + ζ · E
if dvtotal < dvmin then
dvmin ← dvtotal
result ← CPPi
6:
end if
7: end for
8: return result
4:
5:

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

8

In this stage, an adaptive DNN partitioning algorithm is
deployed to divide DNN computations into multiple blocks.
The algorithm is summarized in Algorithm 1. Let Nl represent the number of DNN layers. The inputs to the algorithm
include the workload of each layer wk (k ∈ {1, 2, ..., Nl }),
the execution time of each layer texec
, the transmission time
k
of each layer ttrans
, the execution energy consumption of
k
each layer eexec
, the transmission energy consumption of
k
each layer etrans
, and expected sliced number L (L ≤ Nl ).
k
This algorithm traverses all DNN layers to obtain multiple
partition points. During this search, computing the inference delay and energy consumption of each DNN block
is crucial for selecting partition points that minimize the
overall delay and energy consumption of the collaborative
computing stage. Furthermore, to ensure a balanced workload among resource-constrained satellites, it is necessary
to uniformly distribute the workload across task blocks to
avoid overloading specific satellites and prevent others from
being underutilized. This stage aims to minimize workload variance across subtasks during partitioning, enabling
flexible offloading to satellites with heterogeneous resource
capacities.
As shown in Line 1 of Algorithm 1, the algorithm first
initializes a set that contains all feasible Combinations of
Partition Points (CPP) for partitioning DNN models. Each
combination in the CPP set consists of unordered sequences
of partition points with a length of L − 1, generated through
permutations and combinations. These sequences represent
potential partitioning configurations for the models. For instance, CPPL−1
denotes the (L−1)-th partition point of the ii
th combination within CPP. By considering all possible combinations, this step minimizes the time required to identify
the optimal partition points. Additionally, variance (VAR) is
defined to quantify the workload balance of DNN blocks.
Total delay T and energy consumption E are introduced
to comprehensively evaluate the influence of execution and
transmission latency and energy consumption on the selection of partition points.
Then, our algorithm iterates all combinations of partition points (Line 2 in Algorithm 1) to identify the optimal
partition points. For each combination, the DNN model is
divided into several DNN blocks according to the specified
partition points. The workload variance (Eq. 3), total latency
(Eq. 9), and total energy consumption (Eq. 13) of these DNN
blocks are calculated within the simulated satellite environment. A weighted sum of these three metrics is then computed (Line 4 in Algorithm 1). The partition points yielding
the smallest combined metric value are selected as optimal.
The final result, which includes the set of optimal partition
points is returned. It is important to note that APT-SAT is
primarily designed for targets DNN-based models such as
ResNet and VGG using horizontal partitioning; extending
its applicability to other architectures requires additional
modifications, such as modeling cross-layer dependencies
for Transformer-based models.
The time complexity of finding the optimal partition
point combination is dependent on the number of partition
points, denoted as O(Nl L−1 ), where L − 1 represents the
number of partition points. For each partition combination,
the time complexity of dividing the DNN model based
on the partition points is determined by the size of the

slices. However, the cumulative complexity remains O(Nl ).
Therefore, the overall time complexity of Algorithm 1 is
O(Nl · Nl L−1 ), equivalently O(Nl L ). In terms of space
complexity, the only additional memory required is for
storing the indicators of DNN layers, resulting in a space
complexity of O(Nl ). Furthermore, a multi-stage approach
can be introduced to alleviate the computational complexity when applying this algorithm to a large scale model.
This approach initially partitions the network coarsely (e.g.,
into large segments), followed by independent refinement
within each segment, effectively pruning the search space
while maintaining relatively high-quality solutions.

4.4

Collaborative Computing Stage

After conducting Algorithm 1, the arriving DNN tasks are
split into L segments and distributed into several blocks.
Subsequently, it becomes necessary to establish a satellite
processing sequence. In this stage, we propose a routing
and task offloading algorithm that utilizes SAC due to
its capability for continuous exploration and adaptive task
assignments. This adaptability is particularly valuable for
effectively managing real-time fluctuations and uncertainties in dynamic satellite networks.
Since the offloading decision at the time slot t is influenced by the decision at the previous time slot t − 1, the
computation offloading process in the assumed dynamic
scenario can be constructed as a Markov Decision Process
(MDP). The MDP is defined as M = (S, A, P, R, γ), where
the elements S , A, P , R and γ represent the state space,
action space, state transition matrix, reward function, and
discount factor, respectively. Each element of the MDP is
detailed as follows.
State space S : Each satellite is equipped with a predefined computational resource margin and a maximum communication range referred to as the maximum interaction
distance. At each time step t, the state st is constructed
by combining two components: (i) a tensor ct1 , ct2 · · · , ctn ,
representing the percentage of remaining computational
resources of all satellites within the current satellite’s maximum communication range, and (ii) the ratio wt , which
denotes the size of the current task slice relative to the
satellite’s maximum computational resource capacity. At
each time slot t, the state st can be defined as:

st = {ct1 , ct2 , · · · , ctn , wt }.

(19)

Action space A: After determining st , the corresponding
discrete and continuous actions are generated. The current
satellite is referred to as the central satellite, which can
select the next satellite to offload the arriving task slices.
The potential candidates for offloading include all satellites
within the maximum communication range, as well as the
central satellite itself if it chooses to continue executing the
next task slice locally. In the assumed satellite environments,
the scale of the satellites is pre-defined, and a coordinate
system is established with the first satellite at the bottom-left
corner as the reference point. The position of each satellite
is represented in the form (xi , yi ). Consequently, the action
space consists of the coordinates of all satellites within the

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

9

maximum communication range. At each time slot t, the
action at can be defined as:

at = {(xt1 , y1t ), (xt2 , y2t ), · · · , (xtn , ynt )}.

(20)

In the action space, we assume that resource allocation is
contingent upon the offloading decision, i.e., the offloading
decision takes precedence over resource allocation in the
decision-making hierarchy. Therefore, due to the limited
computational resources available on satellites, there may
be cases where the allocated resources are insufficient to
execute the current task slice, rendering the action invalid.
In such cases, the task will be dropped (See Eq. 14).
State Transition Probability Matrix P : The transition
probability from state st to st+1 can be represented as
q(st+1 |st , at ). Since st+1 is influenced by at and both the
state space s and the action space A are large, it is challenging to accurately model q(st+1 |st , at ). Therefore, we adopt
a model-free approach for computational offloading [55].
Reward Function R: After performing action at under
state st in time slot t, the system transitions to a new state
st+1 and returns the corresponding reward rt . The reward
function is designed to reflect the cost incurred during the
system state transition, which depends on both the objective
function and the associated constraints. The reward function
rt is defined as

rt = ℜ − ϖOt − ρσ t − ϑ,

(21)

where ℜ is a constant that ensures the reward tends to
be positive. Ot represents the optimization objective (See
Eq. 15) in one time slot. ϖ and ρ are the scaling coefficient.
σ t denotes the variance difference in the distribution of
resource states around the central satellite prior to and
following action selection. ϑ is the penalty incurred during
task offloading when the designated satellite lacks sufficient
computational resources to execute the current task slice,
resulting in task failure.
We introduce an entropy-regularized reinforcement
learning based on SAC to optimize task offloading decisions. In this structure, the agent receives a reward r at
each time step t, which is proportional to the entropy of
the policy at that time. The primary objective of SAC is to
not only optimize the cumulative expected rewards but also
the expected entropy of the policy as:
!
X
π ∗ = arg max Eπ
r(st , at ) + αH(πt (·|st )) , (22)
π

∀t

where α is the parameter that determines the relative importance of the entropy term versus the reward. H(πt (·|st ))
is the entropy calculated by:
X
H(πt (·|st )) = −
πt (at |st ) log (πt (at |st )) .
(23)
a

SAC utilizes double Q-networks and, during each update, selects the network that produces the smaller Q-value
to mitigate the issue of Q-value overestimation. A summary
of these networks is provided below:
Actor network µ: The actor network is responsible for
exploring and selecting actions based on the learned policy,
which influences the agent’s behavior in the multi-agent
environment. The input includes the batch size, the number

Algorithm 2 Routing and Task Offloading Algorithm
Input: w, L, Sa , ssrc ⊂ Sa , π .
Output: The task offloading result scheme.
1: Initialize scheme = [].
2: while Length(scheme) < L do
3:
Calculate action space As for ssrc by Eq. 20
4:
Obtain state S of satellites within ssrc ’s connection
by Eq. 19.
5:
Sample act A ∼ π(·|S; θ) based on S and As .
6:
The destination satellite sdst ← CalDst(A)
7:
Available paths LP ← topk (DF S(ssrc , sdst )) ▷ top
k -shortest paths that can support the current task slice
8:
if LP is None then
9:
Drop the task slice.
10:
return scheme ← ∅
11:
end if
12:
Select ls ← OptBW (LP) by Eq. 26.
13:
Offload w to sdst via ls .
14:
scheme ← task offloading result
15:
Update the computational resource usage of
ssrc , sdst .
16:
for ∀i (satellite) traversed by ls do
17:
Update the communication resources usage.
18:
end for
19: end while
20: return scheme

of intersections and the state. By evaluating the probability
of each action being executed, the training process aims to
maximize the overall state value V π (s), calculated as:

V π (s) = Eπ (Qπ (st , at ) + αH(πt (·|st ))) ,

(24)

where Qπ (st , at ) is the Q-function expressed by:

Qπ (st , at ) = Eπ (r(st , at , st+1 ) + γV π (st+1 )) .

(25)

The actor network outputs the probability of the available actions as πθ (at |st ) and its respective entropy value as
Hπ (at |st ).
Critic network Q: The critic network is designed to
guide the correctness of the behavior. The critical Q-network
estimates the action value, while the target critic network
indicates an estimation of the state value. To stabilize the
training process, the target critic network is updated less
frequently than the primary critic Q-network. Different from
the actor networks, the output of the critic network is the
value of Q function as q1 (st+1 , at ) and q2 (st+1 , at ).
Algorithm 2 outlines the procedure that leverages a welltrained SAC policy network to facilitate offloading decisions. The inputs to the algorithm include the workload of
one task segment w, the expected sliced number L, the set
of indices of available satellites Sa , the initial source satellite
ssrc ⊂ Sa , the well-trained policy net π , and all parameters.
The process begins by selecting ssrc from Sa to execute the
first task slice of w. The offloading destination is determined
by π (See Eq. 22).
Then, ssrc computes the corresponding action space As
by Eq. 20 (Line 3 in Algorithm 2). The state S is obtained
from the satellites in the action space As of ssrc (Line 4
in Algorithm 2). Subsequently, the algorithm uses the policy

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

10

network π to dynamically select an action A from As (Line 5
in Algorithm 2). Based on action A, it retrieves the corresponding destination satellite sdst (Line 6 in Algorithm 2).
In the satellite network, multiple paths may exist between ssrc and sdst , each with varying communication
resource utilization status. In the routing process, the algorithm initially employs depth-first search (DFS) to identify
all feasible transmission paths between ssrc and sdst . From
these candidate paths, the top k -shortest paths that can
accommodate the current task slice are selected, forming
the set LP (Line 7 in Algorithm 2). If no such paths exist,
the task is dropped, and an empty scheme is returned.
The optimal path is determined according to OptBW (LP)
function defined as:

OptBW (LP) = max{min(l1 ), min(l2 ), · · · , min(lk )}.
(26)
Here, {l1 , l2 , ..., lk } denotes the remaining available communication resources allocated to the k paths in LP , where
each path consists of multiple hops. The algorithm identifies
the hop within each path with the minimum communication
resources, then selects the path ps corresponding to the hop
with the highest resource availability among these minima.
This process ensures the identification of the optimal path
(Line 12 in Algorithm 2). The task slice is then offloaded via
ps , and the computational resources of ssrc and sdst , as well
as the communication resources in ps , are updated accordingly. In scenarios involving dynamic satellite topologies,
such as fluctuating resource availability or temporary variations in ISL bandwidth, APT-SAT ’s SAC-based offloading
algorithm reroutes tasks without requiring re-partitioning,
thereby achieving offloading decisions promptly.
The time complexity of the proposed task offloading
algorithm is primarily determined by the number of task
slices L, the size of the available satellite set |Sa |, the computation of the top k paths between satellites, the number
of orbits No in satellite network and the number of satellites
Ns on an orbits. Besides, let Nisl denote the number of ISLs.
So the overall complexity is O(L · (|Sa | + d + k · (|No · Ns | +
|Nisl | + h))), where d is the number of satellites within the
maximum communication distance, and h is the average
hop count of paths.

TABLE 3
M AIN EXPERIMENTAL PARAMETERS
Parameter
Network topology N (size = N × N )
Satellite bandwidth B
Satellite computation capability Cx (Ux )
Satellite transmission power ps
Gateway bandwidth B0
Effective capacitance coefficient κ
Size of computation data of tasks dl
Task workload cl
System time slot length τ
Task arrival rate λ
Task splitting number L
Maximum communication distance DM
Entropy weight α
Reward correction factor ℜ
Reward scaling coefficient ω, ρ
Punishment factor ϑ

TABLE 4
D ETAILS OF SAC ARCHITECTURE
Net

Layer

Units

Activation

Actor

Input
Fully connected
Fully connected
Output

State dimensions
256
256
Discrete action dimensions

–
–
–
Softmax

Critic

Input
Fully connected
Fully connected
Fully connected
Output

State and Action dimensions
256
256
256
1

–
ReLU
ReLU
ReLU
–

function applied to the output layer for discrete actions,
respectively, to ensure the bounds of the output actions. The
critic network comprises a three-layer fully connected structure, where each layer is activated using the ReLU function
to approximate the nonlinear Q value of the output. We
implement two commonly used DNN models, ResNet101
and VGG19 for performance validation. We verify the effectiveness of our proposed APT-SAT against the following
four comparative baseline methods:
•

•

5

E XPERIMENTS

5.1

Experimental Setup

We develop a simulated satellite network environment and
conduct extensive experiments to validate the effectiveness
of the proposed APT-SAT. The experimental setup consists
of a CPU-based server with 16 GB 4800-MHz DDR5, 2.50GHz Intel Core i5.
Considering the regularity of the satellite constellation
topology (network size = N × N ), the neighbors of each
satellite are defined as four adjacent satellites capable of
direct communication. The task generated on each UE is
subject to a Poisson distribution π(λ), where λ indicates
the task arrival rate. Other main parameters used in the
experiments are summarized in Table 3. The architectures
of the actor and critic networks used in SAC are shown in
Table 4. The actor network consists of two fully connected
layers designed to fit the behavior strategy, with a Softmax

Value
4 ∼ 32
20 MHz [54]
3 GHz [48]
30 dBw [48]
10 MHz [48]
10−28 [48]
[200, 300] KB
[2, 2.5] Kcycles/bit
0.05
4 ∼ 40
3 (VGG19), 4 (ResNet101)
3 (VGG19), 3 (ResNet101)
0.01
3
1.5, 1
0.5

•

•

Residual-Resource-Priority (RRP) selects the available satellites with the most residual computing resources to process the next segment of the tasks [56].
Random Offloading Location Assignment (ROLA)
is a method where the candidate satellite for task
offloading is independently and randomly selected.
SCC is a self-adaptive task offloading method developed from a genetic algorithm with the objective
to minimize the task drop rate, delay and energy
consumption [47].
Deep Deterministic Policy Gradient (DDPG) is a
DDPG-based task offloading method with the objective to minimize the system energy consumption
subject to delay and resource constraints [48].

To ensure a fair comparison, we use the parameter configurations specified in the original papers of the baseline
methods. The effectiveness of each method is evaluated
using the following four key metrics: task completion rate
(Eq. 14), total average delay (Eq. 9), total average energy
consumption (Eq. 13), and the variance (Eq. 3) in the total
workload assigned to each satellite.

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

11

(a) Average delay

(b) Energy consumption

(c) Resource usage variance

Fig. 4: DNN partitioning performance achieved by different methods with ResNet101.

(a) Average delay

(b) Energy consumption

(c) Resource usage variance

Fig. 5: DNN partitioning performance achieved by different methods with VGG19.
TABLE 5
TASK COMPLETION RATE WITH DIFFERENT NETWORK SCALES .
Model
ResNet101

VGG19

5.2

Method
ROLA
RRP
DDPG
APT-SAT
ROLA
RRP
DDPG
APT-SAT

16
62.5%
55.7%
64.4%
66.5%
58.6%
52.9%
62.8%
65.9%

64
79.0%
75.1%
77.4%
80.5%
70.1%
72.9%
78.2%
82.8%

144
76.0%
72.4%
83.0%
84.5%
72.2%
68.1%
77.0%
80.9%

Network Scale
256
400
576
74.5%
73.1%
72.5%
72.0%
67.9%
67.0%
80.8%
85.0%
83.3%
82.8% 87.4% 86.3%
69.8%
66.1%
66.5%
66.2%
64.9%
64.7
78.4%
77.5%
77.4%
81.5% 82.6% 83.9%

DNN Partitioning Performance

To investigate the impact of different DNN partitioning
methods on the experimental results, we compare the proposed APT-SAT against four baselines: EO-SAT, which offloads DNN subtasks to a satellite node without partitioning
the model; RPT-SAT, which randomly selects partitioning points; SO-SAT, using single-point partitioning based
on DNN slice execution latency and energy consumption;
and DO-SAT, employing dual-point partitioning under the
same criteria. Figs 4 and 5 show the average delay, energy
consumption, and resource usage variance associated with
each task under different task arrival rates. The results
demonstrate that APT-SAT consistently achieves low delay and energy consumption while maintaining high resource usage by leveraging an adaptive DNN partitioning
method. This method holistically considers the impact of
both task execution delay, energy and transmission delay,
energy on the collaborative computing stage. In contrast,
SO-SAT and DO-SAT adopt single-point and dual-point
partitioning mechanisms, respectively. In complex satellite
network environments, especially when the currently allo-

784
74.3%
68.5%
83.8%
86.8%
66.8%
66.6%
78.4%
82.0%

1024
71.9%
68.4%
83.6%
86.3%
67.2%
64.4%
76.4%
83.8%

6400
73.1%
69.2%
83.8%
86.9%
67.5%
65.2%
77.1%
83.4%

10000
72.6%
68.6%
82.9%
87.2%
67.1%
65.7%
77.5%
84.2%

catable resources vary across satellites, these methods may
lead to increased computational and transmission delays,
as they lack the ability to achieve fine-grained control over
latency and energy consumption. EO-SAT, on the other
hand, does not partition tasks and directly offloads them
for execution. This approach results in significantly higher
computational latency, energy consumption, transmission
latency, and transmission energy consumption.
Since the baseline methods do not inherently incorporate a DNN partitioning stage, the same DNN partitioning
strategy used in APT-SAT is applied to these methods for
subsequent evaluations.
5.3

Overall Performance

We explore the impact of different task arrival rates while
fixing the satellite scale to 10 × 10. The overall performance of different methods, evaluated under two DNN
models ResNet101 and VGG19 is shown in Figs. 6 and 7.
Among them, Figs. 6(a) and 7(a) plot the task completion
rate performance, which shows a gradual decline as the
task arrival rate increases. Specifically, APT-SAT exhibits

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

12

(a) Task completion rate

(b) Average delay

(c) Energy consumption

(d) Resource usage variance

Fig. 6: Overall performance achieved by different methods with ResNet101.

(a) Task completion rate

(b) Average delay

(c) Energy consumption

(d) Resource usage variance

Fig. 7: Overall performance achieved by different methods with VGG19.
the capability to maintain relatively high performance, even
under high task arrival rate conditions. This robustness
can be attributed to its dynamic and adaptive action selection mechanism combined with efficient path planning,
which enables APT-SAT to explore a broader solution space.
Consequently, it achieves approximately a 25% higher task
completion rate compared to alternative methods.
Figs. 6(b)-(c) and Figs. 7(b)-(c) show the average delay
and energy consumption comparison per task, respectively.
As task arrival rate increases, the resources available for task
offloading decrease, leading to a higher average number of
communication hops in the generated paths. This, in turn,
leads to increased delays and energy consumption.
Among the evaluated methods, APT-SAT maintains
a relatively low delay and energy consumption performance by optimizing both transmission and computation
delay/energy consumption during the routing and task
offloading process. In contrast, RRP prefers to select the
fittest satellites, leading to an imbalanced distribution where
a particular satellite is chosen by multiple decision-making
satellites. For DDPG, owing to its insufficient exploration
capability, it is unable to generate superior offloading decisions, and thus the delay and energy consumption are
increased when the task arrival rate is relatively high.
Similarly, SCC struggles to adapt to dynamic network conditions, further increasing delay and energy costs. ROLA,
relying on random selection within the action space, incurs
substantial delays and energy usage due to its lack of
strategic decision-making. In overall, for ResNet101 model,
APT-SAT reduces the average delay by 330 ms, 170 ms,
100 ms, and 60 ms, and the average energy consumption by
0.43 J, 0.27 J, 0.15 J, and 0.07 J compared to RRP, ROLA, SCC,

and DDPG, respectively. For the VGG19 model, APT-SAT
reduces the average delay by 360 ms, 150 ms, 80 ms, and
40 ms, and the average energy consumption by 0.36 J, 0.22 J,
0.08 J, and 0.02 J against the same methods.
Figs. 6(d) and 7(d) highlight the resource usage variance
performance of different methods. In terms of the variance
in satellite usage, A lower variance value of satellite usage
indicates a more robust consideration of load balancing
across different satellites. With the assistance of adaptive
DNN partitioning (Line 3 in Algorithm 1) and reward
function configuration (See Eq. 21), APT-SAT effectively
harnesses the available resources, i.e., APT-SAT can achieve
a similar performance compared with ROLA, which can
theoretically achieve uniform distribution in resource allocation.
5.4

Impact of Network Scale

We evaluate the task completion rate under varying network
scales, with the task arrival rate set to 40 to examine the
robustness of the proposed method under high task load
conditions. As illustrated in Table 5, it is critical to accommodate a heavy task load when the network scale is small
(N = 4). However, with the increase of network scale, the
task completion rate increases significantly for all methods.
Especially, the results demonstrate that APT-SAT can still
outperform other methods even if the network scale is more
than 10,000, i.e., 100×100). This superior performance can
be attributed to APT-SAT’s capability to prioritize satellites
that offer higher rewards, which indicates the selected satellite has more available resources. Furthermore, APT-SAT
effectively minimizes variance in the resource distribution
among adjacent satellites before and after the offloading

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

13

decision. These features collectively enable a more balanced
task distribution and efficient offloading across the network.
When the network scale is 1,024, for ResNet101 model,
APT-SAT improves the task completion rate by 17.9%,
14.3%, 5.1%, and 2.7% compared to RRP, ROLA, SCC, and
DDPG, respectively. Similarly, for VGG19 model, APT-SAT
achieves enhancements of 19.4%, 16.7%, 8.5%, and 7.5% over
RRP, ROLA, SCC, and DDPG, respectively.
5.5

Parameter Sensitivity Analysis

Fig. 8 demonstrates the effect of ϖ and ρ in reward function
rt (Eq. 21) on our proposed APT-SAT using ResNet101. It is
essential to analyze how variance affects the main reward
function by adjusting ρ while fixing the value of ϖ. The
results show that neglecting the impact of variance (ρ = 0)
drastically worsens the performance. When ρ is close to ϖ,
the trend remains similar for all criteria. However, slightly
degraded performance in task completion ratio occurs for
relatively high (= 1.5) or low (= 0.5) ρ values. To ensure
balanced performance, we choose ϖ = 1.5 and ρ = 1 for
the main experiment.

(a) Task completion rate

(b) Resource usage variance

Fig. 8: Overall performance of APT-SAT achieved with different ρ values when ϖ=1.5

6

C ONCLUSION

In this paper, we propose an adaptive DNN partitioning
and task offloading framework APT-SAT within satellite
computing environments. In this framework, we initially
develop delay and energy consumption prediction models,
serving as the basis for DNN partitioning. Then, we develop
an adaptive DNN partitioning algorithm leveraging delay
and energy consumption predictions along with satellite
resource distribution, enabling the division of corresponding DNN tasks into multiple segments. Finally, the tasks
collected by the satellites are split accordingly, and a routing and task offloading algorithm is utilized to determine
optimal offloading sequences for the task slices, after which
the tasks are executed. Experimental results show that our
proposed APT-SAT can outperform other methods across a
range of metrics in diverse network scales and two DNN
models. Specifically, APT-SAT effectively minimizes task
execution and transmission delay, as well as energy consumption, while maintaining a high task completion rate
and ensuring efficient load balancing in satellite resource
allocation. As a future work, we plan to integrate other

satellite characteristics, including inter-satellite interference
and time-varying communication delays for interference
forecasts in the prediction stage.

R EFERENCES
[1]

C.-X. Wang, X. You, X. Gao, X. Zhu, Z. Li, C. Zhang, H. Wang,
Y. Huang, Y. Chen, H. Haas, J. S. Thompson, E. G. Larsson, M. D.
Renzo, W. Tong, P. Zhu, X. Shen, H. V. Poor, and L. Hanzo, “On the
road to 6G: Visions, requirements, key technologies, and testbeds,”
IEEE Communications Surveys & Tutorials, vol. 25, no. 2, pp. 905–
974, 2023.
[2] Y. Gong, H. Yao, J. Wang, M. Li, and S. Guo, “Edge intelligencedriven joint offloading and resource allocation for future 6G
industrial Internet of Things,” IEEE Transactions on Network Science
and Engineering, vol. 11, no. 6, pp. 5644–5655, 2024.
[3] Q. Zheng, J. Jin, Z. Shen, L. Wu, I. Ahmad, and Y. Xiang, “Distributed task processing platform for infrastructure-less iot networks: A multi-dimensional optimisation approach,” IEEE Transactions on Parallel and Distributed Systems, vol. 35, no. 12, pp. 2392–
2404, 2024.
[4] G. Chen, S. Wu, Y. Deng, J. Jiao, and Q. Zhang, “VLEO satellite
constellation design for regional aviation and marine coverage,”
IEEE Transactions on Network Science and Engineering, vol. 11, no. 1,
pp. 1188–1201, 2024.
[5] Z. Shen, J. Jin, C. Tan, A. Tagami, S. Wang, Q. Li, Q. Zheng, and
J. Yuan, “A survey of next-generation computing technologies in
space-air-ground integrated networks,” ACM Computing Surveys,
vol. 56, no. 1, 2023.
[6] F. Wang, H. Yao, W. He, H. Chang, X. Xin, and S. Guo, “Timesensitive scheduling mechanism based on end-to-end collaborative latency tolerance for low-earth-orbit satellite networks,” IEEE
Transactions on Network Science and Engineering, vol. 11, no. 6,
pp. 5149–5162, 2024.
[7] Q. Tang, R. Xie, Z. Fang, T. Huang, T. Chen, R. Zhang, and F. R. Yu,
“Joint service deployment and task scheduling for satellite edge
computing: A two-timescale hierarchical approach,” IEEE Journal
on Selected Areas in Communications, vol. 42, no. 5, pp. 1063–1079,
2024.
[8] X. Luo, D. Liu, H. Kong, S. Huai, H. Chen, and W. Liu, “You
only search once: On lightweight differentiable architecture search
for resource-constrained embedded platforms,” in Proceedings of
the ACM/IEEE Design Automation Conference (DAC), pp. 475–480,
2022.
[9] T. Mohammed, C. Joe-Wong, R. Babbar, and M. D. Francesco,
“Distributed inference acceleration with adaptive dnn partitioning
and offloading,” in Proceedings of the IEEE Conference on Computer
Communications (INFOCOM), pp. 854–863, 2020.
[10] L. Zeng, X. Chen, Z. Zhou, L. Yang, and J. Zhang, “CoEdge:
Cooperative dnn inference with adaptive workload partitioning
over heterogeneous edge devices,” IEEE/ACM Transactions on Networking, vol. 29, no. 2, pp. 595–608, 2021.
[11] Y. Chen, Q. Zhang, Y. Zhang, X. Ma, and A. Zhou, “Energy and
time-aware inference offloading for DNN-based applications in
LEO satellites,” in Proceedings of the IEEE International Conference
on Network Protocols (ICNP), pp. 1–6, 2023.
[12] S. Chen, Y.-C. Liang, S. Sun, S. Kang, W. Cheng, and M. Peng,
“Vision, requirements, and technology trend of 6G: How to tackle
the challenges of system coverage, capacity, user data-rate and
movement speed,” IEEE Wireless Communications, vol. 27, no. 2,
pp. 218–228, 2020.
[13] Y.-C. Tsai and C.-H. Lu, “Direct edge-to-edge attention-based
multiple representation latent feature transfer learning,” IEEE
Transactions on Automation Science and Engineering, pp. 1–14, 2024.
[14] Y. Zhou, R. Zhang, J. Liu, T. Huang, Q. Tang, and F. R. Yu,
“A hierarchical digital twin network for satellite communication
networks,” IEEE Communications Magazine, vol. 61, no. 11, pp. 104–
110, 2023.
[15] X. Zhang, J. Liu, R. Zhang, Y. Huang, J. Tong, N. Xin, L. Liu,
and Z. Xiong, “Energy-efficient computation peer offloading in
satellite edge computing networks,” IEEE Transactions on Mobile
Computing, vol. 23, no. 4, pp. 3077–3091, 2024.
[16] H. H. Esmat, B. Lorenzo, and W. Shi, “Toward resilient network
slicing for satellite–terrestrial edge computing IoT,” IEEE Internet
of Things Journal, vol. 10, no. 16, pp. 14621–14645, 2023.

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

This article has been accepted for publication in IEEE Transactions on Network Science and Engineering. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TNSE.2025.3585287

14

[17] W. Fan et al., “Joint DNN partition and resource allocation for
task offloading in edge–cloud-assisted IoT environments,” IEEE
Internet of Things Journal, vol. 10, no. 12, pp. 10146–10159, 2023.
[18] Z. Zhao, K. M. Barijough, and A. Gerstlauer, “DeepThings: Distributed adaptive deep learning inference on resource-constrained
IoT edge clusters,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 37, no. 11, pp. 2348–2359, 2018.
[19] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and
L. Tang, “Neurosurgeon: Collaborative intelligence between the
cloud and mobile edge,” ACM SIGARCH Computer Architecture
News, vol. 45, no. 1, pp. 615–629, 2017.
[20] J. Mao, X. Chen, K. W. Nixon, C. Krieger, and Y. Chen, “MoDNN:
Local distributed mobile computing system for deep neural network,” in Proceedings of the Design, Automation & Test in Europe
Conference & Exhibition (DATE), pp. 1396–1401, 2017.
[21] N. Shan, Z. Ye, and X. Cui, “Collaborative intelligence: Accelerating deep neural network inference via device-edge synergy,”
Security and Communication Networks, vol. 2020, no. 1, p. 8831341,
2020.
[22] N. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, L. Jiao,
L. Qendro, and F. Kawsar, “DeepX: A software accelerator for lowpower deep learning inference on mobile devices,” in Proceedings
of the ACM/IEEE International Conference on Information Processing
in Sensor Networks (IPSN), pp. 1–12, IEEE, 2016.
[23] L. Zhou, M. H. Samavatian, A. Bacha, S. Majumdar, and R. Teodorescu, “Adaptive parallel execution of deep neural networks on
heterogeneous edge devices,” in Proceedings of the ACM/IEEE
Symposium on Edge Computing, pp. 195–208, 2019.
[24] D. J. Pagliari, R. Chiaro, E. Macii, and M. Poncino, “CRIME: Inputdependent collaborative inference for recurrent neural networks,”
IEEE Transactions on Computers, vol. 70, no. 10, pp. 1626–1639, 2020.
[25] S. Zhang et al., “DeepSlicing: Collaborative and adaptive CNN
inference with low latency,” IEEE Transactions on Parallel and
Distributed Systems, vol. 32, no. 9, pp. 2175–2187, 2021.
[26] E. Li et al., “Edge AI: On-demand accelerating deep neural network inference via edge computing,” IEEE Transactions on Wireless
Communications, vol. 19, no. 1, pp. 447–457, 2019.
[27] H. Kim, J. S. Choi, J. Kim, and J. H. Ko, “A DNN partitioning framework with controlled lossy mechanisms for edge-cloud
collaborative intelligence,” Future Generation Computer Systems,
vol. 154, pp. 426–439, 2024.
[28] H. Li, X. Li, Q. Fan, Q. He, X. Wang, and V. C. M. Leung,
“Distributed DNN inference with fine-grained model partitioning
in mobile edge computing networks,” IEEE Transactions on Mobile
Computing, vol. 23, no. 10, pp. 9060–9074, 2024.
[29] Z. Liao, W. Hu, J. Huang, and J. Wang, “Joint multi-user DNN
partitioning and task offloading in mobile edge computing,” Ad
Hoc Networks, vol. 144, p. 103156, 2023.
[30] H. Liang, Q. Sang, C. Hu, D. Cheng, X. Zhou, D. Wang, W. Bao, and
Y. Wang, “DNN surgery: Accelerating DNN inference on the edge
through layer partitioning,” IEEE Transactions on Cloud Computing,
vol. 11, no. 3, pp. 3111–3125, 2023.
[31] N. Olsen, G. Albini, J. Bouffard, T. Parrinello, and L. ToffnerClausen, “Magnetic observations from CryoSat-2: calibration and
processing of satellite platform magnetometer data,” Earth, Planets
and Space, vol. 72, no. 48, 2020.
[32] B. Denby and B. Lucia, “Orbital edge computing: Nanosatellite
constellations as a new class of computer system,” in Proceedings of
the International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS), pp. 939–954, 2020.
[33] D. Bhattacherjee, S. Kassing, M. Licciardello, and A. Singla, “Inorbit computing: An outlandish thought experiment?,” in Proceedings of the ACM Workshop on Hot Topics in Networks, pp. 197–204,
2020.
[34] S. Wang and Q. Li, “Satellite computing: Vision and challenges,”
IEEE Internet of Things Journal, vol. 10, no. 24, pp. 22514–22529,
2023.
[35] P. Cassará, A. Gotta, M. Marchese, and F. Patrone, “Orbital edge
offloading on mega-LEO satellite constellations for equal access to
computing,” IEEE Communications Magazine, vol. 60, no. 4, pp. 32–
36, 2022.
[36] Z. Lai, Q. Wu, H. Li, M. Lv, and J. Wu, “Orbitcast: Exploiting megaconstellations for low-latency earth observation,” in Proceedings of
the International Conference on Network Protocols (ICNP), pp. 1–12,
IEEE, 2021.
[37] E. Rapuano, G. Meoni, T. Pacini, G. Dinelli, G. Furano, G. Giuffrida, and L. Fanucci, “An FPGA-based hardware accelerator for

CNNs inference on board satellites: benchmarking with Myriad
2-based solution for the CloudScout case study,” Remote Sensing,
vol. 13, no. 8, p. 1518, 2021.
[38] B. Denby and B. Lucia, “Orbital Edge Computing: Nanosatellite
constellations as a new class of computer system,” in Proceedings of
the International Conference on Architectural Support for Programming
Languages and Operating Systems, p. 939–954, 2020.
[39] S. Wang and Q. Li, “Satellite computing: Vision and challenges,”
IEEE Internet of Things Journal, vol. 10, no. 24, pp. 22514–22529,
2023.
[40] L. Cheng et al., “Dynamic computation offloading in satellite edge
computing,” in Proceedings of the IEEE International Conference on
Communications (ICC), pp. 4721–4726, 2022.
[41] Q. Tang, Z. Fei, B. Li, and Z. Han, “Computation offloading in
LEO satellite networks with hybrid cloud and edge computing,”
IEEE Internet of Things Journal, vol. 8, no. 11, pp. 9164–9176, 2021.
[42] Y. Zhang, H. Zhang, K. Sun, J. Huo, N. Wang, and V. C. Leung,
“Partial computation offloading in satellite-based three-tier cloudedge integration networks,” IEEE Transactions on Wireless Communications, vol. 23, no. 2, pp. 836–847, 2023.
[43] B. Mao, F. Tang, Y. Kawamoto, and N. Kato, “Optimizing computation offloading in satellite-UAV-served 6G IoT: A deep learning
approach,” Ieee Network, vol. 35, no. 4, pp. 102–108, 2021.
[44] Z. Ji, S. Wu, and C. Jiang, “Cooperative multi-agent deep reinforcement learning for computation offloading in digital twin satellite
edge networks,” IEEE Journal on Selected Areas in Communications,
vol. 41, no. 11, pp. 3414–3429, 2023.
[45] H. Li, J. Yu, L. Cao, Q. Zhang, Z. Song, and S. Hou, “Multiagent reinforcement learning based computation offloading and
resource allocation for leo satellite edge computing networks,”
Computer Communications, vol. 222, pp. 268–276, 2024.
[46] Y. Lyu, Z. Liu, R. Fan, C. Zhan, H. Hu, and J. An, “Optimal
computation offloading in collaborative LEO-IoT enabled MEC: A
multiagent deep reinforcement learning approach,” IEEE Transactions on Green Communications and Networking, vol. 7, no. 2, pp. 996–
1011, 2022.
[47] S. Peng, X. Hou, Z. Shen, Q. Zheng, J. Jin, A. Tagami, and J. Yuan,
“Collaborative satellite computing through adaptive DNN task
splitting and offloading,” in Proceedings of the IEEE Symposium on
Computers and Communications (ISCC), pp. 1–6, 2024.
[48] H. Zhang, R. Liu, A. Kaushik, and X. Gao, “Satellite edge computing with collaborative computation offloading: An intelligent deep
deterministic policy gradient approach,” IEEE Internet of Things
Journal, vol. 10, no. 10, pp. 9092–9107, 2023.
[49] M. Jia, L. Zhang, J. Wu, Q. Guo, G. Zhang, and X. Gu, “Deep
multiagent reinforcement learning for task offloading and resource
allocation in satellite edge computing,” IEEE Internet of Things
Journal, vol. 12, no. 4, pp. 3832–3845, 2025.
[50] Y. Zhang, H. Zhang, K. Sun, J. Huo, N. Wang, and V. C. M.
Leung, “Partial computation offloading in satellite-based three-tier
cloud-edge integration networks,” IEEE Transactions on Wireless
Communications, vol. 23, no. 2, pp. 836–847, 2024.
[51] Z. Ji, S. Wu, and C. Jiang, “Cooperative multi-agent deep reinforcement learning for computation offloading in digital twin satellite
edge networks,” IEEE Journal on Selected Areas in Communications,
vol. 41, no. 11, pp. 3414–3429, 2023.
[52] H. Li, J. Yu, L. Cao, Q. Zhang, Z. Song, and S. Hou, “Multiagent reinforcement learning based computation offloading and
resource allocation for LEO satellite edge computing networks,”
Computer Communications, vol. 222, pp. 268–276, 2024.
[53] M. Ouyang, R. Zhang, B. Wang, J. Liu, T. Huang, L. Liu, J. Tong,
N. Xin, and F. R. Yu, “Network coding-based multipath transmission for LEO satellite networks with domain cluster,” IEEE Internet
of Things Journal, vol. 11, no. 12, pp. 21659–21673, 2024.
[54] I. Leyva-Mayorga, B. Soret, and P. Popovski, “Inter-plane intersatellite connectivity in dense LEO constellations,” IEEE Transactions on Wireless Communications, vol. 20, no. 6, pp. 3430–3443, 2021.
[55] T. Ren, Z. Hu, J. Niu, W. Feng, and H. He, “M3OFF: Modulecompositional model-free computation offloading in multienvironment MEC,” in Proceedings of the IEEE Conference on Computer Communications (INFOCOM), pp. 2249–2258, 2024.
[56] Y. Zhang, C. Chen, L. Liu, D. Lan, H. Jiang, and S. Wan,
“Aerial edge computing on orbit: A task offloading and allocation
scheme,” IEEE Transactions on Network Science and Engineering,
vol. 10, no. 1, pp. 275–285, 2023.

Authorized licensed use limited to: NANJING UNIVERSITY OF AERONAUTICS AND ASTRONAUTICS. Downloaded on July 14,2025 at 01:09:40 UTC from IEEE Xplore. Restrictions apply.
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

